{"lecture_id": "os5del15", "chunk_id": "os5del15_0000", "start": 0.0, "end": 179.0, "token_count": 593, "text": "Pipelining. Jeg nevnte at en i instruksjonen... Deles opp i flere biter. Man klarer ikke å gjøre den på én klokkesyklus. Det er rett og slett for mange operasjoner som skal gjennomføres, så man har funnet ut at det er bedre å dele opp instruksjonene i små biter enn å ha en... Veldig lang syklus. Så i de aller første mikkelproseksjonene var det bare én syklus, sånn som i vår simulering. Så begynte man ganske snart med to sykler, altså fetch og execute. Først hente instruksjonen, og så utføre den. Etter hvert ble det en sånn standard som hadde de viktigste operasjonene. Det er instruksjonsdekoderen som da finner ut hvilke knapper man skal trykke på i Alu datapapp for å utføre den kommende instruksjonen. Og det må dekodes. Og så kommer execute. Det er virkelig å utføre instruksjonen. Hvis det er å legge sammen to tall, så skjer dette inni Alu. Og så skrive resultatet deres om. Det er jo ikke alltid alle deler som utføres. Det er én måte å dele opp instruksjoner på. Men Moderne CPU-er har enda flere stages enn dette her. 14 stages, eller 14 deler, da, er vanlig. Og da deles da det som vi har tenkt på som én instruksjon, en ad f.eks., det deles da opp i 14 små biter. Så på en måte... Det er detaljer av den institusjonen som skal utføres. Og denne inndelingen gjøres for at alt skal gå fortere, og at man sparer tid. For det man kan gjøre når man har delt opp, når man har den type pipelining, det er at institusjonene kan gjøres samtidig. Altså det vi kan tenke hvis vi har fetch og execute. Hvis vi bare har to stages. Det utføres to stadier, er det vel på norsk. Så har vi første stadiet å hente institusjonen. Og andre er å utføre. Da kan vi tenke oss at når vi er i gang med å utføre den første institusjonen, så kan neste institusjon hentes av Fetch. For da slipper vi å vente på at den hentes. Det er det som er pipelining.", "source": "lecture"}
{"lecture_id": "os5del15", "chunk_id": "os5del15_0001", "start": 138.56, "end": 314.98, "token_count": 561, "text": "det er at institusjonene kan gjøres samtidig. Altså det vi kan tenke hvis vi har fetch og execute. Hvis vi bare har to stages. Det utføres to stadier, er det vel på norsk. Så har vi første stadiet å hente institusjonen. Og andre er å utføre. Da kan vi tenke oss at når vi er i gang med å utføre den første institusjonen, så kan neste institusjon hentes av Fetch. For da slipper vi å vente på at den hentes. Det er det som er pipelining. Operasjoner samtidig. Så hvis vi har fire stadier, så kan vi tenke oss at uten pipelining, så er den første illustrert her. Den blå er en institusjon, den røde er en annen. Så går klokka bortover sånn. Så kan vi tenke oss... Da er vi sånn som i vår simulering, så gjør vi jo én institusjon på hvert klokkesyklus. En CPU som må ha fire sykler for å utføre en institusjon, så ser vi at den første fetch, decode, execute og så wright. Og så kommer neste institusjon, fetch, decode, execute og right. Men pipelining vil si at man gjør to operasjoner samtidig. Man begynner på den neste institusjonen før den første er ferdig. Da får vi denne rekkefølgen istedenfor åtteklokkesykler. Da henter vi først den første institusjonen, og så på neste syklus dekodes den første institusjonen. Men samtidig så hentes den neste, og så videre. Og det er opplagt at da går ting raskere. Ja, her ser vi noen sånne mikroarkitekturer for Intel. Som jeg sa, de aller første... 1986 var en av de første CPU-ene for PC-er. Da var det en pipeline med to stages. Så ser vi at den har økt en del oppover. Noen 54-knyttblader hadde enormt mange stages. Men så har man gått litt tilbake, og i de mer moderne er det 14 stages som er vanlige. Så dette er pipelining. Det gjør at vi gjør på en måte én og én institusjon, men at man tjuvstarter på neste institusjon før den første.", "source": "lecture"}
