{"lecture_id": "linux10del2", "chunk_id": "linux10del2_0000", "start": 0.0, "end": 206.28, "token_count": 592, "text": "Men først må vi spørre som alltid om hvorfor virtualisering? Hvorfor begynte man med dette i det hele tatt? Generelt så må vi gå kanskje 20 år tilbake hvor det typiske var at man hadde fysiske servere. Og da hadde man gjerne én stor fysisk server med mange CPU-er. Og den kjørte da gjerne mange forskjellige tjenester. Du kunne ha én fysisk server som kunne ha en webserver for én bedrift, og den kunne ha en database for en annen, osv. Og da var det mange driftsproblemer som dukket opp pga. denne metoden. F.eks. hvis det var noe feil i hardware, så ville alt gå ned. Hvis man skulle oppgradere, En av tjenestene så måtte man kanskje ta ned de andre også. Spesielt hvis man skulle sette inn mer ram fordi en database trengte mer ram. Så måtte hele serveren tas ned. Og da begynte man å eksperimentere med virtuelle maskiner. På den måten at man kunne fordele ressursene for de fysiske maskinene på mange... Da er det noen punkter vi skal gå gjennom her, som viser hvilke fordeler virtualisering gir. Det er isolasjon, ressurssparing, fleksibilitet, programvareutvikling og skytjenester. Alt dette gjelder også for containere og dokker, som vi har sett på. Dokker og konteinere har generelt blitt en mer vanlig måte å fordele tjenester og servere på, enn å bare bruke rene VM-er. Eller mer vanlig. Begge brukes mye, men konteinere har økt veldig i bruk de siste årene. Men jeg sier, bortsett fra din første, isolasjon og sikkerhet, som... Som jeg sa, der har dokker et større problem. Det er... En dokkecontainer er ikke like godt isolert som en virtualmaskin. Ja... Isolasjon. En fordel ved virtualisering er at da kan man enkelt sette opp tjenester som kjører bare på én V. Og kun på den VM-en. Dermed unngår man at de forskjellige tjenestene ødelegger for hverandre. Man kan gjøre dette med fysiske servere også. Man kan tenke seg at det gjorde man til en viss grad. Man setter opp én dedikert server for en webtjeneste, f.eks.", "source": "lecture"}
{"lecture_id": "linux10del2", "chunk_id": "linux10del2_0001", "start": 165.5, "end": 357.2, "token_count": 600, "text": "Ja... Isolasjon. En fordel ved virtualisering er at da kan man enkelt sette opp tjenester som kjører bare på én V. Og kun på den VM-en. Dermed unngår man at de forskjellige tjenestene ødelegger for hverandre. Man kan gjøre dette med fysiske servere også. Man kan tenke seg at det gjorde man til en viss grad. Man setter opp én dedikert server for en webtjeneste, f.eks. Og så får den webserveren alle ressursene. Men da får man straks et ressursproblem, for da må man fordele veldig mye ressurser på den ene webserveren. Men man får opplagt en fordel med isolasjon. Men dette får man samtidig med virtualisering, for da får man isolasjon, samtidig som man bruker mindre ressurser. Men så kan man spørre om seg Er ikke det et stort problem med virtualisering? Det viser seg at det meste av nedetid skyldes ikke hardware, at det går noe galt med hardware, men heller software. Software for en hypervisor er generelt mindre komplekst enn all programvaren som kjører på en maskin. Du har kompliserte applikasjoner. Avhengigheter av databaser og eksterne programmer osv. Så er det mye mer komplekst enn en hypervisor. En hypervisor er relativt enkelt og relativt stabilt. Sikkerhet er viktig her. Hvis én tjeneste blir hacket, så vil det ikke påvirke de andre tjenestene. Man har ganske vanntette skott mellom to virtuelle maskiner som kjører på den samme serveren. Det er fordi, som jeg nevnte, at operativsel med applikasjonen kommuniserer kun mot det virtuelle hardware-API-et som HyperWise gir dem tilgang til, og ikke noe annet. Så de har på en måte overhodet ikke tilgang til det underliggende hardwaren. De kan kun snakke med API-et og bare be om akkurat det HyperWise ønsker å tilby. Ressurssparing er et veldig viktig moment for virtualisering. Man kan oppnå isolasjon og sette én fysisk server for hver tjeneste, men det gir store driftskostnader. Med virtualisering så kan man oppnå det samme på én enkelt server. For da kan du ha mange VM-er som kjører på den samme serveren.", "source": "lecture"}
{"lecture_id": "linux10del2", "chunk_id": "linux10del2_0002", "start": 313.92, "end": 509.8, "token_count": 593, "text": "Så de har på en måte overhodet ikke tilgang til det underliggende hardwaren. De kan kun snakke med API-et og bare be om akkurat det HyperWise ønsker å tilby. Ressurssparing er et veldig viktig moment for virtualisering. Man kan oppnå isolasjon og sette én fysisk server for hver tjeneste, men det gir store driftskostnader. Med virtualisering så kan man oppnå det samme på én enkelt server. For da kan du ha mange VM-er som kjører på den samme serveren. Og de kan dele på CPU og minne og alt annet. Man kan også enkeltkonsolidere. Det vil si... Hvis man har en fysisk server som man ser at... Oi, her er det mye plass og ressurser. Så kan man flytte Wemer til den serveren. Det er umulig uten virtualisering. Når jeg snakker om virtualisering, så kommer jeg til å nevne en del... Vi har en mastergrad som tidligere het Nettverk og systemarbeidersadministrasjon ved HiOA. Den heter nå Croll Computing. Hvor det har vært tidligere studenter som har jobbet med oppgaver relatert til virtuelle maskiner. På Teaching Materials ligger det en oversikt over Hvis dette er noe som dere finner interessant, så sjekk gjerne ut hva som er gjort her. Og sjekk ut Asit, som er den nymasteren som kjører på Oslo Nett. Skybaserte tjenester er det vel hett generelt. Neste punkt er fleksibilitet. Det er... Alt blir veldig mye mer fleksibelt når man bruker VM-er. Hvis man da bruker container i tillegg, så blir det enda mer fleksibelt. Men når man kjører VM-er, så kan man dynamisk tildele CPU-er og internminer til VM-er. F.eks. de Linux-VM-ene som dere har. I tidligere år så hadde de ofte én enkel CPU. Men nå har dere fire CPU-er. Det er virtuelle CPU-er som er tildelt VM-ene. Så når dere kjører et Java-program med fire tråder, så vil de effektivt kjøre i parallell på fire forskjellige CPU-er. Da bruker de de underliggende CPU-ene som den fysiske serveren har. Da er det også lagt opp sånn at da kan det f.eks. være...", "source": "lecture"}
{"lecture_id": "linux10del2", "chunk_id": "linux10del2_0003", "start": 474.28, "end": 651.8, "token_count": 598, "text": "I tidligere år så hadde de ofte én enkel CPU. Men nå har dere fire CPU-er. Det er virtuelle CPU-er som er tildelt VM-ene. Så når dere kjører et Java-program med fire tråder, så vil de effektivt kjøre i parallell på fire forskjellige CPU-er. Da bruker de de underliggende CPU-ene som den fysiske serveren har. Da er det også lagt opp sånn at da kan det f.eks. være... Åtte VM-er på én server, og hver av de har fire virtuelle CPU-er. Og det blir 32 til sammen. Mens de fysiske maskinene har kanskje bare 16. Men de deler hele tiden dynamisk på CPU-ene. Så det kan også bety at hvis absolutt alle grupper er inne og kjører og bruker 100 % CPU, så vil ting faktisk gå saktere. Det vil se ut som du har 100 % CPU, men i virkeligheten så deler du CPU med andre grupper og andre VM-er. Det samme gjelder i clouden. Hvis du leier VM-er på Amazon eller i Google, så vil den samme effekten kunne oppstå, at ting faktisk går saktere. Fordi du deler CPU med andre VM-er. Ja, det har kanskje noen av dere sett også hvis en VM... Noen av dere har fått ødelagt en VM eller kompromittert. Det var faktisk en gruppe her som hadde lagd en testjuicer. En bruker som het Testjuicer, og sannsynligvis et litt dårlig passord på det. Så de fikk straks besøk fra SSO og innlogging fra et par stykker fra USA. Og en fra Tyskland. Og maskinen var blitt kompromittert. Det som var fint da, var at da kunne vi bare ta ned maskinen og bygge den opp på nytt. Sånn at alt av det gamle var slettet. Generelt, hvis man først har hatt noen inne på en server, så er det veldig vanskelig å garantere seg mot at noe har blitt liggende der, eller at de har med hensikt lagt igjen bakdører. Den eneste måten er å bygge på nytt. Det er veldig enkelt hvis man bruker virtuelle maskiner. Da kan man ta det tidligere imaget man hadde, eller bygge et nytt image. Og så bude opp det. Så har man en ny server på ti minutter.", "source": "lecture"}
{"lecture_id": "linux10del2", "chunk_id": "linux10del2_0004", "start": 617.04, "end": 839.92, "token_count": 584, "text": "Generelt, hvis man først har hatt noen inne på en server, så er det veldig vanskelig å garantere seg mot at noe har blitt liggende der, eller at de har med hensikt lagt igjen bakdører. Den eneste måten er å bygge på nytt. Det er veldig enkelt hvis man bruker virtuelle maskiner. Da kan man ta det tidligere imaget man hadde, eller bygge et nytt image. Og så bude opp det. Så har man en ny server på ti minutter. Det kan være pga. avhengighet fra operativsystemet eller andre programmer. Hvis du nå utvikler alt på en VM, så kan hele VM-en flyttes over. Litt på samme måte som vi har sett på med dokkercontainere. En annen ting man kan gjøre med VM-er, det er å flytte de levende. Man kan ha en virtuell maskin som kjører en tjeneste, som en webserver. Og så kan man da, mens webserveren kjører, så kan man flytte den virtuelle maskinen fra én fysisk server til en annen. Og måten man får den til å være oppe helt inne på, er at man starter med å kopiere det meste. Og så er det et lite øyeblikk til slutt hvor man kopierer akkurat de siste bitene. Det kan gå på millisekunder. Man kan ha flytting av hele fysiske VM-er. Nei, ikke fysiske. Flytting av virtuelle maskiner. Det kan gjøres med nedetid på 10-50 millisekunder. Tidligere masterjunter som har jobbet med... Altså med live migration av virtuelle maskiner. Ja, dette er fra... Ja, dette er... Den neste grafen er fra oppgaven til Bilal. Skalering. Vertikal og horisontal skalering. Det betyr at man da legger til både flere VM-er, men også flere CPU-er og mer minne til en enkelt VM. Så det han gjorde, var altså at han satte opp en webtjeneste. Hvis responstiden økte, så var systemet sånn at den la til flere CPU-er. Oppover her så ser du antall CPU-er. Vi kan bare veldig kort se på antall CPU-er for webserver 1. Her ser vi responstiden ligger under en grense, kanskje 500 millisekunder.", "source": "lecture"}
{"lecture_id": "linux10del2", "chunk_id": "linux10del2_0005", "start": 788.06, "end": 1011.3, "token_count": 598, "text": "flere VM-er, men også flere CPU-er og mer minne til en enkelt VM. Så det han gjorde, var altså at han satte opp en webtjeneste. Hvis responstiden økte, så var systemet sånn at den la til flere CPU-er. Oppover her så ser du antall CPU-er. Vi kan bare veldig kort se på antall CPU-er for webserver 1. Her ser vi responstiden ligger under en grense, kanskje 500 millisekunder. Men når responstiden begynner å komme opp mot den grensen og så går over, så ser du at dynamisk så legger systemet til flere CPU-er. Hele tiden mens responstiden går oppover, så legges det til CPU-er. Helt til man har kommet så høyt at responstiden går ned igjen. Så går responstiden opp enda en gang, at man får enda mer pågang på webserveren. Og da er systemet dynamisk på den måte at den legger til en ny webserver, som også får flere CPU-er. Sånn at disse to webserverne til sammen kan håndtere trafikken her. Og en fleksibilitet som dette vil være helt umulig med fysiske maskiner. Programvareutvikling, det har vi sett på med dere. Det er mye å gjøre. Man kan enkelt sette opp nøyaktig det miljøet man ønsker å kjøre i. Og så kan man teste ut, feilteste osv. Så det er veldig enkelt og raskt å sette opp akkurat det miljøet man ønsker. De av dere som tar skytjenester og nettverkskurset, har jo sett dette i praksis. Man kan da kjøpe seg en VM med et gitt antall CPU-er, gitt diskstørrelse og størrelse på minnet. Det koster typisk mer jo flere CPU-er og mer ramme du har. Veldig store besparelser, og gjør at dette kan være en effektiv måte å selge servertid til alle som ønsker å kjøre tjenester. Og at Amazon er store skytjenester, det skyldes opprinnelig at de var en bokhandel, opprinnelig, som solgte bøker. Da måtte de kjøpe inn masse fysiske maskiner for å kunne ta unna alle request fra kunder. Men da opplevde jeg at de andre elleve månedene i året sto de der", "source": "lecture"}
{"lecture_id": "linux10del2", "chunk_id": "linux10del2_0006", "start": 960.0, "end": 1056.34, "token_count": 271, "text": "Veldig store besparelser, og gjør at dette kan være en effektiv måte å selge servertid til alle som ønsker å kjøre tjenester. Og at Amazon er store skytjenester, det skyldes opprinnelig at de var en bokhandel, opprinnelig, som solgte bøker. Da måtte de kjøpe inn masse fysiske maskiner for å kunne ta unna alle request fra kunder. Men da opplevde jeg at de andre elleve månedene i året sto de der med en masse fysiske maskiner som ingen brukte, som bare ble til overs. I mellomtiden, når det ikke var jul, kunne vi ikke leie ut disse fysiske serverne. Å leie ut fysiske servere er ikke så lett, men ved hjelp av virtualisering og skytjenester så kunne vi sette opp et opplegg hvor de da leide ut ressurser som var tilgjengelige resten av året. Så var på en måte sånn kommersielle skytjenester startet. Det er fantastisk å se hvordan folk oppfører seg og hvordan de oppfører seg.", "source": "lecture"}
