{"lecture_id": "os5del16", "chunk_id": "os5del16_0000", "start": 0.0, "end": 198.0, "token_count": 599, "text": "Ja, her ser vi noen sånne mikroarkitekturer for Intel. Som jeg sa, de aller første... 8086 var en av de første CPU-ene for PC-er. Da var det en pipeline med to stages. Så ser vi at den har økt en del oppover. Så har man gått litt tilbake. Og i det mer moderne så er det 14 stages som er vanlige. Så dette er pipelining. Det gjør at vi gjør på en måte én og én institusjon, men at man tjuvstarter på neste institusjon før den første er ferdig. Så kommer et litt annet prinsipp. Og det er superskalararkitektur. Og her utnyttes både pipelining, det samme prinsippet, men også, så i en superskalararkitektur, så gjøres ting faktisk i parallell. Og det gjøres da ved at man har... La oss si du har åtte instruksjoner etter hverandre. ... seks institusjoner etter hverandre. Da er det kanskje én institusjon som er ad, og én er remove. De er av litt forskjellig type. Da gjør man faktisk de institusjonene, i den grad det er mulig, i parallell. Her kan dere kanskje skimte at det står alu. Her er det to aluer, og så er det en load. Og så er det en alu her borte, med jump osv. Så det som skjer da, er at man... Først så deler man alle de seks institusjonene inn i mikroinstitusjoner. Så hver av de har kanskje 14 mikroinstitusjoner, med fetch-delen her oppe. Og så sender man det til en scheduler. På den måten at de seks institusjonene... Hvis mulig utføres de parallelt på helt uavhengige små aluer her som jobber parallelt med å regne og lagre. Og alt dette er da selvfølgelig bare for å få ting til å gå fortere. For da kan man få til å utføre instruksjoner i løpet av en syklus. Eller kanskje enda raskere. Ved at man... Gjør ting i parallell inne i CPU-en. Dette må ikke forveksles med det vi skal se på senere med multitasking, for da har vi fullstendig store CPU-er som jobber uavhengig av hverandre. Men her deles institusjonene, sekvensielle institusjoner,", "source": "lecture"}
{"lecture_id": "os5del16", "chunk_id": "os5del16_0001", "start": 163.76, "end": 349.92, "token_count": 581, "text": "Og alt dette er da selvfølgelig bare for å få ting til å gå fortere. For da kan man få til å utføre instruksjoner i løpet av en syklus. Eller kanskje enda raskere. Ved at man... Gjør ting i parallell inne i CPU-en. Dette må ikke forveksles med det vi skal se på senere med multitasking, for da har vi fullstendig store CPU-er som jobber uavhengig av hverandre. Men her deles institusjonene, sekvensielle institusjoner, som kommer etter hverandre, opp i små biter. Og så utføres de i parallell. Så her kan det være institusjon 1 som holder på, Alle instruksjoner kan ikke utføres i parallell. Noen er avhengige av hverandre. Men det er et samarbeid mellom kompilatoren og CPU-en. Så prøver man å gjøre så mye som bare mulig parallelt. Jeg skal lære prosessord. Det norske ordet kan kanskje diskuteres... En skalar i motsetning til en vektor. Det er noe som ikke er i parallell. Men iallfall en skalar prosessor. Det utfører institusjonene én og én. De superskalære prosessorene, som egentlig er alle prosessorer etter år 2000, de er superskalære. De har da flere parallelle enheter som utfører mikrooperasjoner. Sånn at de kan jobbe med helt forskjellige institusjoner på én gang, og kjøre de faktisk samtidig. Men det bryr man seg egentlig vanligvis ikke så mye om som operativstem, for man ser ikke dette. CPU-en sørger alltid for at den logikken i koden utføres som den skal. Den kan til og med utføre operasjoner auto-order i en annen rekkefølge. Men da hender det man må gå tilbake og rette opp hvis noe går galt. Her ser vi Intel Core 2, som er en relativt moderne SPU-arkitektur. Her ser vi at den har en masse lag. Den deler instruksjonene opp i mange småbiter. Og så ser vi... Her nede så har vi da i parallell flere aluer. Her er en alu-branch og SSE-alu, og så har vi store address... Store data, load address. Så for hver...", "source": "lecture"}
{"lecture_id": "os5del16", "chunk_id": "os5del16_0002", "start": 295.56, "end": 411.8, "token_count": 326, "text": "Den kan til og med utføre operasjoner auto-order i en annen rekkefølge. Men da hender det man må gå tilbake og rette opp hvis noe går galt. Her ser vi Intel Core 2, som er en relativt moderne SPU-arkitektur. Her ser vi at den har en masse lag. Den deler instruksjonene opp i mange småbiter. Og så ser vi... Her nede så har vi da i parallell flere aluer. Her er en alu-branch og SSE-alu, og så har vi store address... Store data, load address. Så for hver... Hver institusjon kan da deles opp i biter, sånn at det er en pipeline, og samtidig så kan det kjøres i parallell. Det gir da mye større ytelse på prosessoren. Man får da prosessoren til å yte litt ekstra ved at tingene gjøres parallelt. Men det er en grense for hvor mye du kan parallellisere. Sånn som åtte instruksjoner... Utover det er det veldig vanskelig å få til parallellisering. Det er ikke alltid det virker. Det er et samarbeid mellom kompulatoren og CPU-en å få til den optimale kjøringen av en prosess. Det gjøres ved de to metodene pipelining og parallellitet, eller superskalær CPU.", "source": "lecture"}
